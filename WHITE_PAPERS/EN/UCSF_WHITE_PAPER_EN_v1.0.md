---
layout: default
title: UCSF White paper EN v1.0
---

<a id="inhoudsopgave"></a>
# UCSF — Universal Consent & Safety Framework  
## Academic English Version  
**Version:** v1.0  
**Date:** 12 January 2026  
**Status:** Public white paper – open for peer review
---

## Table of Contents

0. [Preface](#0-preface)

1. [Rationale and Necessity](#ch-1)  
   1.1 Structural harm in digital systems  
   1.2 From incident to design pattern  

2. [Consent in Digital Systems](#ch-2)  
   2.1 Consent as a condition, not an action  
   2.2 Free consent, revocability, and dependency  
   2.3 Legal versus ethical legitimacy  

3. [Safety as a Design Responsibility](#ch-3)  
   3.1 Reactive versus preventive safety  
   3.2 Predictable harm and design accountability  

4. [Failure, Recovery, and Repair](#ch-4)  
   4.1 Inevitability of failure at scale  
   4.2 Recovery as obligation, not goodwill  

5. [Power, Governance, and Responsibility](#ch-5)  
   5.1 Platform power and decision-making  
   5.2 Transparency, counter-power, and legitimacy  

6. [Implementation Without Ethical Evasion](#ch-6)  
   6.1 Ethics as a boundary condition  
   6.2 Scalability versus permissibility  

7. [Limits and Ethical Refusal](#ch-7)  
   7.1 Normalisation of structural harm  
   7.2 Not-building as an ethical outcome  

8. [Infrastructure, Material Harm, and Shared Responsibility](#ch-8)  
   8.1 Digital systems as physical infrastructure  
   8.2 Externalisation of ecological and social costs  

9. [Scope and Use](#ch-9)  
   9.1 Normative framework, not certification  
   9.2 Limitations and explicit boundaries  

10. [Authorship and Human Responsibility](#ch-10)

- [Acknowledgements](#acknowledgements)  
- [License and Use](#license-and-use)  
- [Invitation to Peer Review](#invitation-to-peer-review)  
- [Concluding Reflections](#concluding-reflections)  
- [UCSF of Sources](#accessibility-of-sources)  
- [References / Bibliography](#references--bibliography)

---
---

<a id="0-preface"></a>
## 0. Preface

Digital systems have evolved into structures that deeply intervene in human life. They influence identity, livelihood, visibility, safety, and societal participation, often without direct human intervention. Where such systems were once supportive tools, they increasingly function as decisive infrastructure.

The ethical assumptions underlying these systems have not evolved at the same pace. Due to the speed and scale at which digital technologies are developed and deployed — including, at the time of writing, the introduction of AI glasses as an everyday interaction medium — a structural gap has emerged between technical implementation and normative accountability. Ethics is not merely delayed; it is systematically overtaken.

This document proceeds from the observation that much of the harm visible in digital environments is not the result of exceptional failure or misuse, but of predictable design patterns. Consent is reduced to formality, safety is organised reactively, recovery mechanisms are absent or inadequate, and power concentrates without effective counterbalance or transparent accountability.

The Universal Consent & Safety Framework (UCSF) was developed in response to this structural discrepancy. It is neither technology-specific nor a reaction to a single incident. UCSF provides a normative framework for assessing and constraining systems that exert influence over human autonomy, safety, and livelihood.

UCSF adopts a design-oriented ethical position: when harm is predictable, it is not accidental; when recovery is absent, it is not an oversight but a choice; and when consent cannot be freely withdrawn, it does not constitute consent. In this sense, the framework addresses not only what systems do, but what they are permitted to do.

This document does not claim moral completeness nor definitive solutions. It establishes minimal conditions under which digital systems can function in an ethically defensible manner, and explicitly recognises that not building, not scaling, or not deploying a system may constitute a legitimate ethical outcome.

UCSF does not offer comfort.  
It sets limits.

[⬆ Back to table of contents](#inhoudsopgave)

---

<a id="ch-1"></a>
## 1. Rationale and Necessity

This chapter outlines the structural developments and recurring design patterns that give rise to the formulation of the Universal Consent & Safety Framework (UCSF).

Digital systems increasingly make decisions with direct consequences for people. They determine who is visible, who has access to services or income, who is excluded, and who loses their position without clear explanation. These decisions increasingly occur without direct human intervention, based on automated processes, policy logic, and scaling mechanisms that remain largely opaque to those affected.

What renders this development problematic is not technical complexity alone, but the asymmetry it creates. Individuals are confronted with systems that possess agency yet lack accountability. Decisions are often final, difficult to contest, and poorly explained, while their consequences may deeply affect livelihood, reputation, and autonomy.

Across digital contexts — from social platforms to labour marketplaces and AI-driven services — the same structural patterns recur:
- consent reduced to formality or a one-time action;
- safety addressed only after demonstrable harm;
- recovery mechanisms that are slow, insufficient, or absent;
- concentration of power without effective counterbalance or transparent accountability.

These patterns are not incidental. Their recurrence across sectors and technologies indicates shared design choices and underlying assumptions.

UCSF proceeds from the premise that much digital harm is predictable. Where harm occurs structurally, it cannot be dismissed as a side effect or unintended consequence. In such cases, responsibility lies at the design level — particularly when systems are intentionally deployed at scale despite foreseeable risks.

A recurring limitation of existing ethical frameworks is their normative strength paired with operational weakness. Ethics is articulated as intention or aspiration, while system architecture, governance, and implementation remain unexamined. This produces a gap between stated values and actual outcomes.

The Universal Consent & Safety Framework exists to address this gap explicitly. It offers a normative framework for assessing and constraining systems that affect human autonomy, safety, and livelihood.

At its core, UCSF asserts that ethics cannot be separated from design. When systems prevent consent from being freely withdrawn, fail to enable recovery, or render power unchallengeable, these are not usage failures but system properties.

UCSF therefore positions itself as explicitly design-oriented. It shifts attention from individual behaviour or isolated incidents to the systems that enable such outcomes.

This chapter establishes the framework’s normative starting point. Subsequent chapters elaborate its principles across consent, safety, recovery, governance, and implementation, defining minimal conditions for ethical legitimacy.

*(See [1](#ref-1), [2](#ref-2), [5](#ref-5), [6](#ref-6), [7](#ref-7))*

[⬆ Back to table of contents](#inhoudsopgave)

---

<a id="ch-2"></a>
## 2. Consent in Digital Systems

In many digital systems, consent is treated as a one-time action: a click, an agreement, or tacit acceptance through continued use. This approach reduces consent to a legal checkbox and ignores the dynamics of power, dependency, and context in which digital interactions occur.

UCSF rejects this approach.

Consent is not an event, but a condition. It must be explicit, revocable, contextual, and temporally bounded. A system that treats consent as permanent or implicit shifts responsibility unilaterally onto the user and removes itself from ethical scrutiny.

Under UCSF, the following principles apply:
- consent must be actively and unambiguously given;
- consent must be withdrawable without direct or indirect sanctions;
- consent must always be tied to a specific purpose and a specific context;
- silence, continued use, or economic necessity must never be interpreted as consent;
- dependency relationships (financial, social, or infrastructural) must not be used to coerce consent.

### Consent as a Power Relation

Within digital systems, consent cannot be separated from power relations. When access to work, visibility, social participation, or income is made conditional on agreement, consent shifts from free choice to a prerequisite for survival. In such contexts, consent no longer functions as an ethical legitimising mechanism, but as an instrument of discipline.

UCSF therefore holds that consent is meaningful only when refusal remains a real option without disproportionate consequences. Systems that formally request agreement while structurally enforcing dependency undermine the normative value of consent, regardless of legal compliance.

When withdrawal of consent results in loss of visibility, access, income, or livelihood, there is no free consent but structural coercion. In such situations, consent functions as a legitimising tool for power rather than an expression of autonomy.  
*(See [3](#ref-3))*

### 2.1 Consent, Power, and Asymmetry

Digital systems rarely operate in power-neutral contexts. Platforms control access, visibility, enforcement, and economic conditions. Users therefore occupy a structurally dependent position, even when participation appears voluntary.

UCSF holds that consent is invalid when:
- the user has no realistic alternative;
- the consequences of refusal or withdrawal are disproportionate;
- conditions are imposed unilaterally and are non-negotiable;
- the functioning of the system is insufficiently understandable to allow informed choice.

In such cases, consent functions as compliance: not as choice, but as necessary adaptation to retain access. This distinction is essential. A system may be legally compliant while remaining ethically indefensible.

### 2.2 Consent as a Design Responsibility

UCSF explicitly shifts consent from the user interface to system design. It is insufficient to “offer” consent through terms or settings; systems must be designed such that:
- withdrawal of consent is technically possible and practically feasible;
- consequences of withdrawal are proportional and transparent;
- prior consent does not create permanent ownership or power claims;
- consent is not bundled with irrelevant or cumulative conditions.

When a system can only function by structurally undermining or fixing consent, this is not an implementation flaw but a fundamental design failure.

### 2.3 Limits of Permissibility

UCSF recognises that not every system can be ethically redesigned. In some cases, it is impossible to guarantee free, revocable, and meaningful consent without fundamentally altering the system itself.

In such cases, ethical refusal — not building, not deploying, or not scaling a system — is not failure, but a necessary outcome.

Within UCSF, consent is not a formality and not a legal shield. It is a test of legitimacy. Where consent cannot exist freely, a system loses its ethical justification.

[⬆ Back to table of contents](#inhoudsopgave)

---

<a id="ch-3"></a>
## 3. Safety as a Design Responsibility

Many digital systems approach safety reactively. Harm is addressed only after it has occurred, through moderation, reporting mechanisms, or appeal procedures. In this model, safety is treated as an operational correction layer, detached from the system’s underlying design.

UCSF holds that this approach is fundamentally insufficient.

When harm is foreseeable, it must be addressed in advance. Systems that repeatedly produce the same forms of harm cannot appeal to coincidence, exceptional misuse, or unforeseen behaviour. Structural harm points to structural design choices.

Within UCSF, safety is not understood as the absence of incidents, but as a property of system design itself. Safety is therefore not an auxiliary feature, but a core responsibility that must be secured prior to deployment.

### 3.1 Testable Safety

UCSF treats safety not only as a normative principle, but as a testable criterion.  
A system meets the safety requirement only when the following minimum conditions are satisfied:

- harm is not structurally reproducible  
- escalation pathways are bounded and controllable  
- vulnerable users do not bear disproportionate risk  
- correction does not depend on public exposure or reputational damage  
- prevention is demonstrably more effective than post hoc repair  

Designs that cannot meet these criteria fail not technically, but ethically.

### 3.2 Risk versus Harm

UCSF makes an explicit distinction between risk and harm.  
Risk may, under certain conditions, be ethically acceptable; harm is not.

Digital systems may contain uncertainty, but they must not incorporate predictable, structural harm as a functional byproduct. When harm becomes necessary for scalability, profitability, or core functionality, the problem lies not in execution but in the system itself.

Normalising harm under the guise of “complexity” or “inevitable side effects” is unacceptable within UCSF.

### 3.3 Automation and Responsibility

Automation shifts execution, not responsibility.

When decisions are delegated to algorithmic processes or automated infrastructures, moral accountability remains with the human design, governance, and ownership structures. The absence of direct human intervention does not reduce risk; it increases the obligation to secure safety in advance.

Systems that legitimise their impact by referring to automation abstract responsibility in an ethically indefensible way.

### 3.4 Safety as a Relational Design Principle

Within UCSF, safety is relational: it cannot be separated from power relations, dependency, and context. A system that is safe for one group but structurally harmful to another does not meet the safety requirement.

A system that functions safely only when users behave perfectly, possess full knowledge, or remain constantly vigilant is inherently unsafe.

Safety must be carried by the design itself, not offloaded onto individual users.

*(See [4](#ref-4), [9](#ref-9))*

[⬆ Back to table of contents](#inhoudsopgave)

---

<a id="ch-4"></a>
## 4. Failure, Recovery, and Repair

No system functions flawlessly at scale. As digital systems gain greater autonomy and exert deeper influence over human lives, the likelihood of failure increases. The ethical question is therefore not *whether* failure will occur, but *how* systems are designed to deal with failure when it does.

In many digital environments, failure is treated as an exceptional event: a bug, an incident, an individual error. Recovery is framed as a service, a courtesy, or an act of goodwill. This framing obscures the structural nature of harm in large-scale systems.

UCSF rejects the notion that recovery is a favour. When a system is capable of causing harm, recovery is not an optional add-on but an **ethical core responsibility of system design**.

### 4.1 Failure as a Predictable Design Problem

Failure in digital systems is rarely entirely unforeseen. Many harm patterns are repeatable, systematic, and statistically predictable: wrongful exclusion, misclassification, loss of income, reputational damage, or psychological harm.

When such harm occurs repeatedly, it can no longer be classified as incidental. It becomes a **structural effect of system design**.

Under UCSF:
- if harm is foreseeable, it must be accounted for in design;
- systems that structurally produce harm fail ethically, even if they function technically;
- the absence of recovery mechanisms is not neutrality, but a choice.

A system that functions correctly while producing irreparable harm is not ethically neutral but normatively deficient.

### 4.2 Recovery as a Design Requirement

In many systems, recovery mechanisms are added only after deployment, often under pressure from public criticism, regulation, or litigation. UCSF holds that this approach is inadequate.

Recovery must be **designed in advance** and integrated into the system architecture.

This includes, among other things:
- **reversibility** of decisions where possible;
- **proportionality** of measures and sanctions;
- **timeliness** of recovery (harm that is repaired too late remains harm);
- **continuity protection** where livelihood is at stake, such as access to work, income, or visibility.

Where decisions are irreversible, the ethical threshold for deployment must be substantially higher.

### 4.3 Repair versus Correction

Many systems limit recovery to correction: fixing an error, restoring a status, reactivating an account. UCSF explicitly distinguishes between correction and repair.

Correction restores the technical state.  
Repair acknowledges and addresses the **consequences of harm**.

Repair may include:
- compensation for incurred loss;
- restoration of reputation or visibility;
- transparent acknowledgement of error and impact;
- structural modification to prevent recurrence.

Without repair, harm remains externalised and borne by individuals, while the system itself remains intact.

### 4.4 Asymmetry of Power and Burden of Proof

In many digital environments, the burden of proof lies with the user: individuals must demonstrate that harm occurred, that the system erred, and that recovery is justified. At the same time, the system controls the data, logs, algorithms, and decision-making authority.

UCSF considers this asymmetry ethically problematic.

When a system exercises power over people, it must:
- **carry responsibility for evidentiary accountability**;
- **provide transparency regarding decision-making**;
- **guarantee accessible recovery pathways** without excessive barriers.

A recovery procedure that exists in theory but is practically unreachable does not meet ethical standards.

### 4.5 Irreparable Harm and Ethical Limits

Not all harm can be repaired. Where harm is structural, large-scale, or irreversible, UCSF holds that the design itself must be questioned.

When:
- recovery is impossible;
- harm is disproportionate;
- or burdens systematically fall on vulnerable groups;

it is ethically defensible — and sometimes necessary — to refuse deployment, scaling, or continuation.

Recovery is not an alibi for legitimising harmful systems. The absence of recovery, however, is a direct indicator that a system has crossed its ethical limits.

*(See [9](#ref-9), [10](#ref-10))*

[⬆ Back to table of contents](#inhoudsopgave)

---

<a id="ch-5"></a>
## 5. Power, Governance, and Responsibility

Digital systems are not neutral infrastructures. They structure power through control over access, visibility, enforcement, data, and revenue flows. This exercise of power is often implicit, technologically mediated, and removed from direct democratic or legal oversight.

UCSF starts from the premise that ethical responsibility resides where **factual power** is exercised, not where intentions are stated.

Whenever a system can make decisions that affect livelihood, reputation, safety, or autonomy, it exercises power — regardless of whether that power is formal, informal, automated, or described as merely “supportive.”

### 5.1 Power as a System Property

In many digital environments, power is obscured by language of efficiency, usability, or neutrality. Decisions are framed as technical necessities or algorithmic outcomes, while they in fact embody normative choices.

Power manifests, among other things, through:
- control over who is visible and who is not;
- access to markets, platforms, or infrastructure;
- enforcement of rules without symmetrical participation;
- asymmetrical information positions;
- unilateral modification of conditions.

UCSF holds that automation does not dissolve power. Automating decisions does not eliminate responsibility; it **embeds power into design**.

### 5.2 Governance versus Intent

Many organisations invoke good intentions, ethical guidelines, or value statements. UCSF recognises intent as morally relevant, but considers it **ethically insufficient**.

What matters is governance: the concrete structures through which power is exercised, constrained, and reviewed.

Under UCSF:
- intentions without enforceable structures provide no protection;
- values without governance are symbolic;
- ethics without countervailing power are fragile.

Ethical responsibility requires:
- explicit decision-making processes;
- verifiable rules and criteria;
- meaningful avenues for objection and correction;
- transparency regarding who decides and why.

### 5.3 Concentration of Power and Absence of Counterpower

Large-scale digital systems tend toward strong concentration of power. Users, creators, and dependent parties typically possess limited bargaining power and few effective means to contest decisions.

This asymmetry is reinforced by:
- dependence on platform-based income;
- lock-in effects;
- lack of viable alternatives;
- legal and procedural complexity;
- opaque decision-making mechanisms.

UCSF treats the absence of effective counterpower not as market failure, but as an **ethical design failure**.

Systems that concentrate power without structural counterbalance undermine their own ethical legitimacy.

### 5.4 Responsibility and Delegation

A recurring pattern in digital systems is the diffusion or outsourcing of responsibility: decisions are attributed to algorithms, third-party vendors, moderation teams, or “the system.”

UCSF rejects responsibility dispersion where it results in opacity or unaccountability.

Under UCSF:
- responsibility cannot be delegated to systems;
- outsourcing does not remove ethical obligation;
- automation does not reduce accountability.

Where power resides, responsibility follows — regardless of technical architecture or organisational distance.

### 5.5 Transparency as a Necessary but Insufficient Condition

Transparency is often presented as a solution to power asymmetry. While necessary, UCSF considers transparency **insufficient** when it is not paired with agency.

Visibility without influence does not correct power.

Ethical governance therefore requires:
- intelligible transparency (not merely formal disclosure);
- real opportunities for contestation;
- recovery following wrongful decisions;
- structural adjustment when harm recurs.

Transparency without corrective mechanisms legitimises power rather than constraining it.

### 5.6 Power and Ethical Limitation

UCSF holds that not all exercises of power are ethically permissible, even when legal, profitable, or efficient.

When:
- power is structurally asymmetric;
- dependence is not voluntary;
- harm is foreseeable and recurrent;
- and counterpower is absent;

the appropriate response is not optimisation but **limitation**.

In some cases, this requires fundamental restructuring or the ethical discontinuation of the system itself.

*(See [5](#ref-5), [6](#ref-6))*

[⬆ Back to table of contents](#inhoudsopgave)

---

<a id="ch-6"></a>
## 6. Implementation Without Ethical Evasion

Ethical principles most often fail at the point of implementation. It is here that the gap between normative commitment and actual practice becomes most visible.

UCSF holds that ethics is not an abstract layer that hovers above systems, but a design requirement that must be **operationally enforceable** within technical, organisational, and economic decisions.

### 6.1 The Myth of the Implementation Dilemma

Digital systems frequently frame ethical shortcomings as unavoidable byproducts of scale, speed, or complexity. This framing suggests that ethics is a secondary concern that can be addressed once optimisation has been achieved.

UCSF rejects this premise.

When ethical principles apply only insofar as they do not introduce friction, they do not function as principles but as preferences.

Design choices that produce harm because they are deemed “practically necessary” are not ethical dilemmas; they are instances of **ethical evasion**.

### 6.2 Ethical Debt and Technical Debt

Within software engineering, technical debt is a recognised concept: temporary compromises made for expediency that require later remediation. UCSF introduces the concept of **ethical debt**.

Ethical debt arises when:
- known risks are consciously ignored;
- recovery mechanisms are deliberately postponed;
- harm is accepted as an operational cost;
- dependency is structurally exploited.

Unlike technical debt, ethical debt does not accumulate within systems, but within human lives. It manifests as insecurity, exclusion, psychological harm, and long-term structural disadvantage.

### 6.3 Ethics as a Boundary Condition

UCSF positions ethics not as a variable within an optimisation model, but as a **boundary condition**.

Under UCSF:
- systems must be ethically defensible prior to scaling;
- unethical efficiency is not progress;
- profitability does not justify harm.

If a system can function only by violating ethical constraints, then the system itself is untenable.

### 6.4 Institutional Responsibility in Implementation

Ethical implementation requires institutional accountability. Individual engineers, moderators, or end users cannot compensate for structural ethical deficiencies.

UCSF therefore requires:
- explicit allocation of ethical responsibility;
- decision-making authority at organisational level;
- documented recovery and escalation procedures;
- independent oversight where power is concentrated.

Ethics that depends on individual goodwill is fragile and does not scale.

### 6.5 Implementation as Ethical Proof

Under UCSF, ethical intent is not demonstrated through policy statements or aspirational language, but through implementation itself.

A system’s ethical credibility is measured by:
- what it technically allows and disallows;
- how it behaves under stress or failure;
- how easily harm can be corrected;
- who bears the cost when safeguards fail.

Ethics that cannot survive implementation is not ethics, but rhetoric.

*(See [7](#ref-7), [11](#ref-11), [12](#ref-12))*

[⬆ Back to table of contents](#inhoudsopgave)

---

<a id="ch-7"></a>
## 7. Limits and Ethical Refusal

Not all harm is acceptable.  
Not all systems deserve scaling.  
Not all innovation is ethically defensible.

UCSF introduces **ethical refusal** as a legitimate and necessary outcome of ethical analysis, rather than as a failure of innovation or governance.

### 7.1 The Normalisation of Structural Harm

Within digital ecosystems, certain forms of harm are routinely normalised. Exclusion, psychological pressure, economic dependency, and structural inequality are frequently framed as unavoidable side effects of scale or complexity.

UCSF rejects this normalisation.

When harm is:
- structural rather than incidental;
- predictable rather than exceptional;
- recurrent across contexts;
- borne primarily by parties without meaningful choice;

it is not a side effect, but a defining characteristic of the system.

### 7.2 The Ethical Limit of Optimisation

Many systems respond to harm by optimisation rather than elimination. Thresholds are adjusted, frictions reduced, or safeguards incrementally improved, while the underlying design remains intact.

UCSF holds that some harms cannot be resolved within the existing system architecture.

When ethical constraints conflict fundamentally with core functionality, further optimisation does not resolve the ethical problem — it conceals it.

In such cases, continued development constitutes ethical evasion rather than ethical progress.

### 7.3 Refusal as a Valid Outcome

Under UCSF, the following outcomes are ethically legitimate:
- choosing not to build a system;
- halting further scaling or deployment;
- withdrawing or decommissioning an existing implementation.

These outcomes are justified when:
- consent cannot be meaningfully free or revocable;
- recovery is impossible or disproportionate;
- power cannot be effectively constrained;
- harm cannot be prevented without dismantling core mechanisms.

Growth, efficiency, or market demand do not override these conditions.

### 7.4 Refusal as a Design Principle

UCSF treats refusal not as an external veto, but as an internal design option.

Ethical system design includes:
- defining non-negotiable boundaries;
- embedding stop conditions;
- recognising non-market outcomes as valid results.

A system that lacks the capacity for ethical refusal is ethically rigid by design.

### 7.5 Responsibility in Saying “No”

Ethical refusal requires responsibility. Declining to build, scale, or continue a system entails acknowledging trade-offs, opportunity costs, and potential benefits forgone.

UCSF does not deny these costs. It asserts that **some costs are ethically preferable to harm**.

When refusal is avoided solely to preserve growth trajectories, investor confidence, or institutional momentum, ethical responsibility has already been abandoned.

*(See [1](#ref-1), [2](#ref-2), [4](#ref-4))*

[⬆ Back to table of contents](#inhoudsopgave)

---

<a id="ch-8"></a>
## 8. Infrastructure, Material Harm, and Shared Responsibility

Digital systems are often framed as immaterial or virtual. In practice, they rely on extensive physical infrastructures: data centres, energy grids, water resources, mineral extraction, logistics networks, and large-scale human labour.

UCSF holds that ethical responsibility does not end at the user interface.

### 8.1 The Materiality of Digital Systems

Every digital interaction carries material consequences. At scale, these consequences translate into significant environmental and social impact, including energy consumption, water usage, land conversion, resource depletion, and labour exploitation.

These impacts are frequently:
- geographically displaced;
- outsourced to marginalised communities;
- legally fragmented across jurisdictions;
- rendered invisible to end users and decision-makers.

UCSF treats this displacement not as an externality, but as an ethical issue intrinsic to system design.

### 8.2 Externalisation as an Ethical Failure

When systems achieve efficiency, profitability, or scalability by externalising ecological or social costs, they shift harm away from those who benefit toward those with limited capacity to refuse or resist.

Under UCSF, such externalisation constitutes ethical failure when:
- harm is foreseeable;
- affected parties lack meaningful consent or bargaining power;
- benefits and burdens are systematically misaligned.

Legal compliance does not absolve ethical responsibility in these cases.

### 8.3 Shared but Non-Diluted Responsibility

Infrastructure is often distributed across complex supply chains involving multiple actors: platform operators, cloud providers, hardware manufacturers, energy suppliers, and subcontracted labour forces.

UCSF rejects the notion that responsibility dissipates through complexity.

Shared responsibility does not imply diluted responsibility.  
Each actor remains accountable for the ethical implications of their contribution.

Under UCSF:
- knowledge of harm creates responsibility;
- scale increases obligation rather than distance;
- indirect contribution does not negate ethical agency.

### 8.4 Invisible Labour and Human Cost

Beyond environmental impact, digital systems depend on large volumes of human labour, including content moderation, data annotation, system maintenance, and corrective work.

When such labour is:
- structurally underpaid;
- psychologically harmful;
- socially invisible;
- treated as disposable or replaceable;

the system incorporates exploitation as a functional dependency.

Contractual legitimacy does not neutralise ethical concern when labour conditions are shaped by asymmetry, precarity, or lack of alternatives.

### 8.5 Infrastructure and Ethical Legitimacy

A system cannot claim ethical legitimacy solely on the basis of its digital behaviour while ignoring its material footprint.

Under UCSF, ethical assessment must include:
- environmental sustainability across the full lifecycle;
- labour conditions throughout infrastructural chains;
- cumulative and long-term impacts, not only immediate effects.

Systems that systematically externalise material harm undermine their own ethical justification, regardless of user-facing safeguards.

*(See [5](#ref-5), [6](#ref-6), [9](#ref-9), [13](#ref-13))*

[⬆ Back to table of contents](#inhoudsopgave)

---

### 8.6 From Extractive to Regenerative Infrastructure

Most contemporary digital infrastructures operate on an extractive model: value is concentrated centrally while environmental degradation, labour precarity, and social costs are distributed outward.

UCSF explicitly rejects extractive infrastructure as an ethically neutral baseline.

Minimising harm is insufficient when systems are structurally dependent on depletion.

UCSF therefore introduces regenerative infrastructure as a normative direction.

Regenerative infrastructure refers to system architectures that do not merely reduce negative impact, but actively contribute to ecological recovery, social resilience, and local agency.

This includes, but is not limited to:

- preference for renewable or community-owned energy sources;
- transparency regarding data center energy, water use, and lifecycle impact;
- architectural restraint in scale where decentralised or federated models are viable;
- reinvestment in communities hosting physical infrastructure;
- responsible sourcing of materials and hardware;
- protection of human labour from invisibility, precarity, and psychological harm.

Regeneration does not imply perfection.

It implies directional responsibility.

Ethics do not start at interfaces. Their true beginning lies in resource harvesting, supply chains, and in how surrounding environments are treated. Infrastructure is not neutral.

Extraction, by default, is a design Choice.

Systems do not inherit extractive patterns accidentally; they encode them. Where digital architectures depend on depletion without reciprocal responsibility, ethical legitimacy is already compromised.

Under UCSF, systems must demonstrate movement away from extractive dependency toward restorative participation in the environments and communities they affect.

Regenerative infrastructure is therefore not an optional enhancement, but an extension of consent and safety into the material world.

Ethics does not stop at interfaces.

It begins at resource harvesting.

[⬆ Back to table of contents](#inhoudsopgave)

---

<a id="ch-9"></a>
## 9. Scope, Applicability, and Limits of Use

The Universal Consent & Safety Framework (UCSF) does not claim moral completeness, nor does it offer a universal solution to all ethical challenges posed by digital systems.

Instead, UCSF defines **minimum ethical conditions** under which systems that affect human autonomy, safety, or livelihood may be considered ethically defensible.

### 9.1 A Normative, Not Instrumental Framework

UCSF is not a product, certification, checklist, or compliance mechanism.  
It does not function as a guarantee of ethical correctness, nor as a shield against criticism or liability.

UCSF operates as:
- a **normative evaluation framework**;
- a **boundary-setting instrument**;
- a tool for **ethical refusal as well as ethical design**.

Use of UCSF without willingness to accept its limiting conclusions undermines its purpose.

### 9.2 Systems Within Scope

UCSF applies to digital systems that:
- make or enforce decisions;
- regulate access, visibility, or participation;
- create structural dependency;
- have the capacity to cause individual or systemic harm.

The framework is technology-agnostic.  
It focuses on **effects and power relations**, not on labels such as “AI,” “platform,” or “automation.”

### 9.3 Systems Outside Direct Resolution

UCSF explicitly recognises that some contexts:
- require political decision-making rather than technical adjustment;
- exceed the scope of design-level intervention;
- depend on societal consensus rather than system architecture.

In such cases, UCSF does not provide solutions.  
It functions as a **warning framework**, identifying ethical fault lines rather than resolving them.

### 9.4 Ethical Refusal as a Valid Outcome

A central premise of UCSF is that refusal is a legitimate ethical conclusion.

When minimum conditions cannot be met — including:
- free and revocable consent;
- meaningful safety guarantees;
- proportional and accessible repair;
- bounded and contestable power —

continuing implementation is not neutral.  
It becomes an active ethical choice to accept harm.

Under UCSF, the decision **not to build**, **not to scale**, or **not to deploy** is ethically valid and, in some cases, ethically required.

### 9.5 Limits of Interpretation and Use

UCSF may not be:
- selectively quoted to legitimise harmful systems;
- reduced to abstract principles without operational consequence;
- applied retroactively to justify existing damage;
- used as branding, marketing, or ethical signalling.

Any application of UCSF must acknowledge both its requirements **and its boundaries**.

*(See [8](#ref-8), [10](#ref-10), [14](#ref-14), [15](#ref-15))*

[⬆ Back to table of contents](#inhoudsopgave)

---

<a id="ch-10"></a>
## 10. Authorship and Human Responsibility

This document has been authored by a human, with the use of AI systems as tools for structuring, language refinement, and iterative development. Such assistance does not alter authorship, nor does it dilute responsibility for the content.

UCSF is grounded in the fundamental position that **ethics cannot be delegated**.  
Systems may assist, accelerate, or structure human work, but they cannot hold moral responsibility.

### 10.1 AI as Instrument, Not Author

AI systems operate within parameters defined by human choices: training data, objectives, constraints, and deployment contexts. They possess no intent, no moral understanding, and no capacity for ethical judgment.

The use of AI in the creation of this document does not affect:
- the normative positions articulated;
- the ethical boundaries defined;
- the responsibility for consequences arising from interpretation or application.

All ethical claims made within UCSF are human choices and remain human responsibilities.

### 10.2 Responsibility Cannot Be Shifted to Systems

UCSF explicitly rejects the displacement of responsibility onto:
- “the system”;
- “the algorithm”;
- “automation”;
- “emergent behavior” or “unforeseen effects.”

When systems cause harm, responsibility lies with those who:
- designed the system;
- approved its deployment;
- enabled its scaling;
- maintained it despite known risks.

Technical complexity does not absolve moral accountability.

### 10.3 Transparency of Process

Explicitly acknowledging AI assistance is not a weakness but an ethical requirement. In an environment where human and automated decision-making increasingly intersect, clarity about authorship and agency is essential.

UCSF treats this disclosure as a baseline standard for ethical transparency, not as an exception.

### 10.4 Final Position

Ethics cannot be automated.

They require human judgment, responsibility, and willingness to accept consequences.  
Absent these elements, ethical language becomes rhetorical rather than substantive.

[⬆ Back to table of contents](#inhoudsopgave)

---

<a id="acknowledgements"></a>
## Acknowledgements

UCSF emerged from observation, lived experience, and prolonged ethical reflection on digital systems and their impact on human lives. The framework did not arise from a single discipline, a single incident, or a single theoretical model, but from the convergence of technical, societal, and moral questions that are too often treated in isolation.

This document has been shaped by existing academic literature, public debates, and broader societal developments. At the same time, it is explicitly not a summary of existing work, but an independent normative position.

Acknowledgement is due to:
- researchers and thinkers who refuse to reduce ethics to abstraction;
- critical voices who make structural harm visible;
- and all those willing to articulate ethical boundaries, even when doing so is uncomfortable.

UCSF is not a closed project. It is offered for examination, critique, and further refinement, while preserving its core principles.

Responsibility for the content, choices, and formulations of this document rests entirely with the author.

[⬆ Back to table of contents](#table-of-contents)

---

<a id="license-and-use"></a>
## License and Use

This document is made available for academic, societal, and policy-oriented reflection on ethics, consent, and safety in digital systems.

### Permitted Use

This document may be freely:
- read, shared, and cited;
- used for non-commercial educational, academic, and research purposes;
- discussed in public debates, peer review processes, and policy contexts;

provided that proper attribution is given and the content is not selectively or misleadingly represented.

### Restrictions

The Universal Consent & Safety Framework (UCSF):
- is not a certification, quality mark, or compliance label;
- may not be used to implicitly or explicitly present products, platforms, or services as “ethically approved”;
- may not be commercially exploited without explicit permission from the author;
- may not be modified, repackaged, or reframed in ways that undermine its normative position, defined boundaries, or principles of ethical refusal.

The use of UCSF terminology, structure, or language in marketing, branding, or product claims without substantive implementation is explicitly rejected.

### Authorship and Responsibility

The intellectual authorship of this document remains with the author.  
The framework reflects the author’s normative position and design-ethical perspective at the time of publication.

Interpretation, application, or implementation of UCSF within concrete systems remains the sole responsibility of the implementing party. Reference to this document does not exempt any actor from their own ethical, legal, or societal responsibilities.

### Versioning and Development

UCSF is a living framework. Future versions may be published in response to societal developments, peer review, and advancing insight. Earlier versions remain traceable and valid within their original context.

[⬆ Back to table of contents](#table-of-contents)

---

<a id="invitation-to-peer-review"></a>
## Invitation to Peer Review

The Universal Consent & Safety Framework (UCSF) is explicitly offered for open academic, societal, and interdisciplinary review.

Critical analysis, substantive counterarguments, complementary literature, and reflections on applicability are welcomed. Peer review is not approached as validation, but as a necessary process for sharpening normative boundaries and design-ethical premises.

### Who Is Invited to Respond

In particular, responses are welcomed from:
- AI ethicists and philosophers;
- legal scholars and policy experts in technology and human rights;
- designers and engineers of digital systems with societal impact;
- researchers in platform studies, governance, and digital labor;
- civil society organizations working at the intersection of technology and vulnerable groups.

### How to Submit Feedback

Substantive feedback, comments, and peer review contributions may be submitted via:

- **GitHub (preferred):**  
  through issues or pull requests in the public repository where this document is maintained.  
  This ensures transparency, version control, and open discussion.

- **Email:**  
  for extended contributions, formal peer reviews, or cases where public discussion is not appropriate.

### Contact

- **Name:** Martijn Bruzzese  
- **Framework:** Universal Consent & Safety Framework (UCSF)  
- **GitHub:** https://github.com/MartijnBruzzese  
- **Email:** martijnbruzzese@gmail.com

All contributions will be read, documented, and—where relevant—integrated into future versions. Responses are not filtered on agreement, but on substantive quality.

[⬆ Back to table of contents](#table-of-contents)

---

<a id="concluding-reflections"></a>
### Concluding Reflections

This document is written from the conviction that ethical questions surrounding digital systems can no longer be treated as secondary considerations, optimization challenges, or deferred concerns. They constitute the core of the systems themselves.

The Universal Consent & Safety Framework (UCSF) makes explicit that much of the harm currently attributed to “misuse,” “complexity,” or “unintended consequences” is, in fact, the result of recurring design patterns and structural choices. When consent cannot be meaningfully withdrawn, when safety is addressed only after damage occurs, and when repair is not embedded in system architecture, the resulting harm is not accidental but predictable.

UCSF does not introduce new moral ideals. Instead, it articulates minimal ethical conditions. It clarifies where boundaries lie and acknowledges that not every technically feasible system constitutes an ethically defensible implementation. In this sense, UCSF functions both as a framework for evaluation and as a framework for refusal.

Importantly, UCSF makes no claim to completeness or finality. It is not a closed moral system, nor a universal solution. It provides a structure within which responsibility can be assumed, choices can be made explicit, and harm can no longer be dismissed as incidental.

What ultimately distinguishes UCSF is not the complexity of its principles, but the clarity of its central assertion:  
when systems exercise power over human autonomy, safety, or subsistence, ethical responsibility is not an optional layer but a foundational design requirement.

That responsibility cannot be delegated.  
It remains human.

During the development of this framework, it became increasingly evident that the pace at which AI systems are developed and deployed is exceptionally high. Even within the timeframe of this work, significant technological shifts occurred, making it impractical to exhaustively account for every specific innovation or implementation detail.

This acceleration is not peripheral to the problem UCSF addresses; it is central to it. The speed with which AI technologies are introduced—often outpacing normative, legal, and societal reflection—intensifies the need for a framework that does not depend on specific applications or momentary configurations.

Equally significant is the cumulative societal impact of these systems. Their effects extend beyond technical performance and directly shape conditions of autonomy, safety, visibility, and subsistence. This cumulative impact confers not only relevance but urgency upon the present work.

In this sense, UCSF is not merely a theoretical contribution. It is a necessary intervention in a discourse evolving faster than the structures intended to constrain it.

[⬆ Back to table of contents](#table-of-contents)

---

<a id="accessibility-of-sources"></a>
## Accessibility of Sources

The sources used in this document consist of a combination of peer-reviewed academic articles, scholarly books, and public policy documents. Not all referenced materials are freely accessible; this reflects standard practice within academic research.

Where possible, open-access versions have been consulted. In other cases, authoritative editions published by academic or institutional publishers have been used. All sources are fully traceable through academic search engines such as Google Scholar and through university library systems.

The inclusion of non–open-access sources does not imply exclusivity of knowledge, but reflects the current structural realities of academic publishing. The normative arguments advanced in this document do not rely on restricted access for their validity and can be critically assessed independently of paywalled availability.

UCSF does not claim originality through obscurity. All references are provided to enable verification, critique, and further research by interested readers.

[⬆ Back to table of contents](#table-of-contents)

---

<a id="references--bibliography"></a>
<a id="references"></a>
## References / Bibliography

The following sources underpin the normative, ethical, and structural foundations of the Universal Consent & Safety Framework (UCSF).

<span id="ref-1"></span>

[1] Benjamin, R. (2019).  
*Race After Technology: Abolitionist Tools for the New Jim Code*.  
Polity Press.  
— Critical analysis of algorithmic bias, structural inequality, and technological power.

**Source:** [Available here](https://www.ruhabenjamin.com/)

↩ [Back to Chapter 1](#ch-1)  
↩ [Back to Chapter 7](#ch-7)

<hr>

<span id="ref-2"></span>

**[2] Floridi, L., Cowls, J., et al. (2018).**  
AI4People—An Ethical Framework for a Good AI Society. *Minds and Machines*, 28(4), 689–707.  
— European principles for responsible AI development.

**Source:** [Available here](https://link.springer.com/article/10.1007/s11023-018-9482-5)

↩ [Back to Chapter 1](#ch-1)  
↩ [Back to Chapter 7](#ch-7)

<hr>

<span id="ref-3"></span>

**[3] Binns, R. (2018).**  
Fairness in Machine Learning: Lessons from Political Philosophy.  
— Fairness as a normative problem; why “technical fairness” fails without a value framework.

**Source:** [Available here](https://arxiv.org/abs/1712.03586)

↩ [Back to Chapter 2](#ch-2)

<hr>

<span id="ref-4"></span>

**[4] Nissenbaum, H. (2010).**  
*Privacy in Context: Technology, Policy, and the Integrity of Social Life*.  
Stanford University Press.  
— Contextual integrity as a framework for privacy and consent.

**Source:** [Available here](https://www.sup.org/books/law/privacy-context)

↩ [Back to Chapter 3](#ch-3)  
↩ [Back to Chapter 7](#ch-7)

<hr>

<span id="ref-5"></span>

**[5] Gillespie, T. (2018).**  
*Custodians of the Internet: Platforms, Content Moderation, and the Hidden Decisions That Shape Social Media*.  
Yale University Press.  
— Platform power, moderation, and private governance structures.

**Source:** [Available here](https://yalebooks.yale.edu/book/9780300261431/custodians-of-the-internet/)

↩ [Back to Chapter 1](#ch-1)  
↩ [Back to Chapter 5](#ch-5)  
↩ [Back to Chapter 8](#ch-8)

<hr>

<span id="ref-6"></span>

**[6] van Dijck, J., Poell, T., & de Waal, M. (2018).**  
*The Platform Society: Public Values in a Connective World*.  
Oxford University Press.  
— Platformization and structural impact on public values.

**Source:** [Available here](https://global.oup.com/academic/product/the-platform-society-9780190889777)

↩ [Back to Chapter 1](#ch-1)  
↩ [Back to Chapter 5](#ch-5)  
↩ [Back to Chapter 8](#ch-8)

<hr>

<span id="ref-7"></span>

**[7] Gray, M. L., & Suri, S. (2019).**  
*Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass*.  
Houghton Mifflin Harcourt.  
— Digital labor and invisible human costs of AI systems.

**Source:** [Available here](https://books.google.com/books/about/Ghost_Work.html?id=u10-uQEACAAJ)

↩ [Back to Chapter 1](#ch-1)  
↩ [Back to Chapter 6](#ch-6)

<hr>

<span id="ref-8"></span>

**[8] European Union (2022).**  
*Digital Services Act (DSA)* — Regulation (EU) 2022/2065.  
— Platform responsibility and systemic oversight within the EU.

**Source:** [Available here](https://eur-lex.europa.eu/eli/reg/2022/2065/oj)

↩ [Back to Chapter 9](#ch-9)

<hr>

<span id="ref-9"></span>

**[9] IEEE (2019).**  
*Ethically Aligned Design (EAD), Version 2*.  
— Human-centered values, accountability, and design principles for autonomous and intelligent systems.

**Source:** [Available here](https://standards.ieee.org/wp-content/uploads/import/documents/other/ead_v2.pdf)

↩ [Back to Chapter 3](#ch-3)  
↩ [Back to Chapter 4](#ch-4)  
↩ [Back to Chapter 8](#ch-8)

<hr>

<span id="ref-10"></span>

**[10] Pasquale, F. (2015).**  
*The Black Box Society: The Secret Algorithms That Control Money and Information*.  
Harvard University Press.  
— Transparency, accountability, and power asymmetry in algorithmic systems.

**Source:** [Available here](https://www.hup.harvard.edu/books/9780674970847)

↩ [Back to Chapter 4](#ch-4)  
↩ [Back to Chapter 9](#ch-9)

<hr>

<span id="ref-11"></span>

**[11] Jones, N. (2018).**  
How to stop data centres from gobbling up the world’s electricity. *Nature*, 561, 163–166.  
— Energy consumption and scalability challenges of data centers.

**Source:** [Available here](https://www.nature.com/articles/d41586-018-06610-y)

↩ [Back to Chapter 6](#ch-6)

<hr>

<span id="ref-12"></span>

**[12] OECD (2021).**  
*The Environmental Impacts of Artificial Intelligence*.  
OECD Publishing.  
— Environmental impact of AI and large-scale data systems.

**Source:** [Available here](https://www.oecd.org/digital/the-environmental-impacts-of-artificial-intelligence-7babf571-en.htm)

↩ [Back to Chapter 6](#ch-6)

<hr>

<span id="ref-13"></span>

**[13] Crawford, K. (2021).**  
*Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence*.  
Yale University Press.  
— Material costs: resources, labor, infrastructure, and geopolitics of AI.

**Source:** [Available here](https://yalebooks.yale.edu/book/9780300209570/atlas-of-ai/)

↩ [Back to Chapter 8](#ch-8)

<hr>

<span id="ref-14"></span>

**[14] UNESCO (2021).**  
*Recommendation on the Ethics of Artificial Intelligence*.  
— International norms on human rights, governance, and proportionality.

**Source:** [Available here](https://unesdoc.unesco.org/ark:/48223/pf0000380455)

↩ [Back to Chapter 9](#ch-9)

<hr>

<span id="ref-15"></span>

**[15] Foucault, M. (1977).**  
*Discipline and Punish: The Birth of the Prison*.  
Vintage Books.  
— Power, discipline, and systemic control (analytical framework).

**Source:** [Available here](https://books.google.com/books/about/Discipline_and_Punish.html?id=pWv1R2o_PWsC)

↩ [Back to Chapter 9](#ch-9)

---

**Note:**  
UCSF is normative and design-oriented. These references provide academic grounding rather than an exhaustive literature review.

<small><a href="#table-of-contents">↑ back to contents</a></small>
