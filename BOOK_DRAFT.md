---
layout: default
title: Toward Humane AI — Book Draft
---

# MODELHAVEN — The Why and The How

## Working subtitle
Ethical Creative AI: building systems that protect identity, consent and autonomy

---

## Table of Contents

- [Part I — WHY](#part-i--why)
- [Part II — WHAT](#part-ii--what)
- [Part III — HOW](#part-iii--how)
  - [1. Consent systems](#1-consent-systems)
  - [2. Identity safety and resemblance thresholds](#2-identity-safety-and-resemblance-thresholds)
  - [3. Protection of minors](#3-protection-of-minors)
  - [4. Adult mode and ethical adult spaces](#4-adult-mode-and-ethical-adult-spaces)
  - [5. Domestic-violence aware safety features](#5-domestic-violence-aware-safety-features)
  - [6. Kill-switch and sleep-switch mechanisms](#6-kill-switch-and-sleep-switch-mechanisms)
  - [7. Animals and representation](#7-animals-and-representation)
  - [Defending the voiceless](#defending-the-voiceless)
- [Part IV — Risks and Open Questions](#part-iv--risks-and-open-questions)
- [Part V — The Road Ahead](#part-v--the-road-ahead)

## Part I — WHY

I don’t approach this as an academic or a policy maker, and I’m not trying to sound like one. This project grew out of a mix of things: curiosity about what AI can do, concern about how it is already being used, and a simple belief that we can aim higher than “powerful but careless.”

AI is developing at extraordinary speed. We now have systems that can echo human faces, voices and identities with remarkable realism. That capability is impressive — and also significant. It touches something deeply human: how we are seen, how we are represented, and how much control we have over our own image and story.

At the moment, technology is often moving faster than the frameworks around it. Consent, identity protection, the safety of minors, and the right to change your mind are fundamental human concerns, yet they are frequently treated as add-ons instead of design principles.

We see incredible innovation, but we also see people being:

- Copied  
- Sexualized  
- Impersonated  
- Turned into “material” without being asked  

These are not minor side effects. They shape trust, dignity and wellbeing.

This conversation is not only about humans. AI also touches how we represent and treat animals. They are often turned into symbols, props or entertainment without any real reflection on welfare or dignity. While animals do not participate in consent the way humans do, the ethical question is still there: what kind of relationship with living beings are we normalizing through our tools? A humane technology should avoid turning life — human or animal — into something disposable.

This work is not happening in a vacuum, and it is certainly not a solitary effort. There are already many people, projects and communities moving in the right direction. Researchers, engineers, artists, moderators, social workers, lawyers, educators, volunteers — people who give time and energy to defend those who do not have a strong voice of their own.

Some protect children. Some protect animals. Some fight exploitation. Some quietly build safer systems without ever being noticed. They deserve recognition. This book does not claim to replace their work. It stands beside it, adding one more contribution to a much larger collective effort.

The message is not “nobody is doing this.”  
The message is “many people are — and together, we can still do better.”

I don’t believe that this is an unchangeable situation. It isn’t “just how progress works.” The same creativity that builds powerful AI can also build systems that respect boundaries, protect the vulnerable, and make consent something real rather than symbolic. We are not stuck with the first version of these tools; we get to decide what “better” looks like.

This project sits inside that belief. It started as a question and gradually became a framework-in-progress — something I refer to as the **Universal Consent & Safety Framework (UCSF)**. It is still being developed, and it is not published yet. The intention is to release it openly once a full draft is ready, so that others can review it, challenge it and help strengthen it. Openness is part of the point.

I’m not claiming to have solved everything. I’m working through it step by step, learning as I go. But I’m also optimistic. There is real potential here:

- AI that supports creativity without normalizing exploitation  
- AI that enables imagination without erasing consent  
- AI that treats people not as raw material, but as participants  

That hope — that we can design technology with both capability and conscience — is the reason this work exists.

That is the “why.”

---

## Part II — WHAT

This project began as a set of questions, but it has gradually taken shape into something more defined. At its core, it is about building ways of working with AI that take consent, identity and safety seriously from the start, not as an optional extra.

What I am exploring here is not just a single tool or product. It is closer to a framework — a way of thinking and designing. I use the term **Universal Consent & Safety Framework (UCSF)** as a working name for it. "Universal" doesn’t mean perfect or finished; it means that the basic principles should apply wherever AI systems interact with human identity, creativity or intimacy.

In simple terms, this project asks:

- How do we design AI systems that respect people as people, not as raw material?  
- How do we make consent real, traceable and revocable, not symbolic?  
- How do we protect minors without shutting down creativity or imagination?  
- How do we build adult spaces that are ethical instead of exploitative?  
- How do we give people the ability to say "stop" — and be heard by the system itself?  

The framework is made of several connected ideas. Some are ethical, some are technical, and some are practical "how would this actually work?" questions. They include things like:

- Verified consent  
- Identity protection thresholds  
- Safe defaults for minors  
- Kill-switch and sleep-switch mechanisms  
- Platform designs that make harm harder and respect easier  

It is also important to say what this is **not**. It is not a demand that AI should disappear. It is not an attempt to control art or creativity. It is not a moral lecture. It is an effort to create space where powerful tools and human dignity can coexist without one erasing the other.

I don’t believe we are starting from zero. A lot of people, projects and communities are already moving in the right direction. There are developers building safer tools, artists talking about consent, and researchers thinking hard about ethics. That matters. At the same time, I still think we can do better. We can connect these ideas more clearly, make them easier to use in practice, and build systems where respect for people isn’t optional — it’s the default.

The UCSF is still a work in progress. Parts of it are already written down, others are still forming. The goal is eventually to publish it openly for review, discussion and criticism. It isn’t meant to be owned by one person or company. The hope is that it becomes something others can test, challenge, improve, adapt and build on.

So "what is this?"

It is a framework-in-progress for **ethical creative AI** — one that tries to hold capability and conscience together, and that treats consent, safety and identity not as obstacles to progress, but as part of what progress actually means.

---

## Part III — HOW

Part I explained why this matters. Part II described what this project is aiming to be. Part III is about something more practical: how ideas like consent, safety and identity protection can start to take real shape in systems and workflows.

This is not a finished blueprint. It is a direction of travel. The goal is to show that these ideas are not vague ideals — they can be translated into processes, defaults and design choices that actually change outcomes.

### 1. Consent systems

Consent should not be a checkbox that nobody reads. It should be a living part of the system.

In practice, this means exploring things like:

- Clear records of when and how consent was given  
- The ability for someone to withdraw consent later  
- Systems that actually respond when consent is revoked  
- Consent that is specific, not “anything forever”  

Consent that cannot be withdrawn is not really consent.

### 2. Identity safety and resemblance thresholds

AI systems can unintentionally generate people who look very similar to real individuals. That matters.

A practical response involves:

- Detecting when generated identities are too close to real people  
- Setting resemblance thresholds that keep outputs safely fictional  
- Designing systems to “push away” from close matches  
- Treating identity as something that deserves protection, not extraction  

The aim is not perfection, but meaningful distance and respect.

### 3. Protection of minors

Minors deserve the strongest possible default protections. That means:

- No sexualization of minors — not implicitly, not “suggestively,” not at all  
- Clear separation between adult and non-adult spaces  
- Defaults that fail on the side of safety when there is doubt  
- World-building or storytelling involving minors kept non-sexual by design  

The guiding principle is simple: safety first, always.

### 4. Adult mode and ethical adult spaces

Adult content exists and will continue to exist. The question is not whether it should disappear, but whether it can be made more ethical and consent-based.

That involves ideas such as:

- 18+ verification concepts  
- Verified consent for participants  
- Collaboration modes where all involved parties explicitly agree  
- Systems that avoid exploitation and non-consensual use of identities  

The goal is not to shame adult spaces, but to make them safer and more respectful.

### 5. Domestic-violence aware safety features

Technology should not put people at greater risk in real life.

This includes concepts like:

- Silent or “stealth” safety modes  
- Evidence lockers where information can be securely stored  
- Support for both smartphones and basic “dumb” phones  
- Features designed with the reality of domestic violence in mind  

Safety features should not assume ideal circumstances; they should assume real ones.

### 6. Kill-switch and sleep-switch mechanisms

People change their minds. Circumstances change. Systems should recognize that.

Two simple ideas follow from this:

- A **sleep switch** — temporary pause of use or distribution  
- A **kill switch** — permanent withdrawal or deletion of content or consent  

Revocation should not be theoretical. It should do something in the system.

### 7. Animals and representation

Animals cannot give consent in the way humans do, yet they are frequently used in storytelling, advertising and generated content. Ethical design still applies here. It means:

- avoiding depictions that normalize cruelty or neglect  
- being mindful about how animals are instrumentalized purely as “content”  
- recognizing that living beings are not decorative objects  

The goal is simple: creativity should not dull our sense of care toward other forms of life.

### Defending the voiceless

A recurring idea in this project is simple: some beings do not get to speak inside our systems. Children often cannot defend themselves. Animals cannot enter consent forms. Vulnerable people may not be heard even when they do speak. Ethical technology has to take this seriously.

Designing with empathy means:

- assuming not everyone can advocate for themselves  
- building defaults that protect those who are easiest to exploit  
- listening carefully to those who work directly with vulnerable groups  

Defending the voiceless is not sentimentality. It is a design principle.

---

This section is not the final word on “how.” It is the beginning of a toolbox — a set of directions that can be refined, challenged, implemented, and improved over time.

The underlying idea is consistent:

**Ethics are not decorations added afterward. They are part of the engineering.**

---

## Part IV — Risks and Open Questions

No framework about people, technology and ethics is ever simple. It’s important to be honest about the limits, risks and unknowns instead of pretending they don’t exist.

### 1. The risk of unintended consequences

Even well-intentioned systems can have side effects. A safety feature that protects one person might accidentally block another. A consent mechanism might be misunderstood or misused. Any system that touches identity and creativity needs to stay humble, adjustable and open to correction.

The question is not “can we avoid every mistake?”  
The